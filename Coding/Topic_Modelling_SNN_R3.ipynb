{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm4dA5M7Cf4B"
      },
      "source": [
        "## **Topic Modelling using Sequential Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOnPhoKlCrUv"
      },
      "source": [
        "##Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP18u_3I_hWd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import xgboost as xgb\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        ")\n",
        "from keras.utils import np_utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score,classification_report\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg5moK1L_lUx",
        "outputId": "9290c44d-6c10-48e5-9f53-242e92a63727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9IU7pyp_q6x",
        "outputId": "77d9e256-8920-4af6-ba16-26e9bd34466e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 10.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.4\n"
          ]
        }
      ],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "import seaborn as sns\n",
        "import string\n",
        "!pip install Unidecode\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2sIQrbX_ukx",
        "outputId": "1c569ef4-b481-4ada-d614-22b84dfd9eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 89 kB 3.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzXXOQ3G_4wY"
      },
      "outputs": [],
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset # initialize label powerset multi-label classifier\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq_jQsHACzAS"
      },
      "source": [
        "##Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgiHlFbjAGOx"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "WY36YQnmAKFR",
        "outputId": "b0ab47d0-b1b0-43af-9d7f-ff143bd6278f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-982673a0-4f9e-4e8d-aa4a-f8c075d8b665\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-982673a0-4f9e-4e8d-aa4a-f8c075d8b665\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving sample_sub.csv to sample_sub.csv\n",
            "Saving Tags.csv to Tags.csv\n",
            "Saving Test.csv to Test.csv\n",
            "Saving Train.csv to Train.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-mFoBFuATTh"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "test = pd.read_csv(io.BytesIO(uploaded['Test.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYpw8cTNAefa"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "train = pd.read_csv(io.BytesIO(uploaded['Train.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGp6HST8An-y"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "tag = pd.read_csv(io.BytesIO(uploaded['Tags.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcJricLqAnZs"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "ss = pd.read_csv(io.BytesIO(uploaded['sample_sub.csv']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4fVO8g5C3yG"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X7fbYFzA8Na",
        "outputId": "5e3a4ef4-a0e6-4584-e4ae-1f8476afbfce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14004 entries, 0 to 14003\n",
            "Data columns (total 31 columns):\n",
            " #   Column                                        Non-Null Count  Dtype \n",
            "---  ------                                        --------------  ----- \n",
            " 0   id                                            14004 non-null  int64 \n",
            " 1   ABSTRACT                                      14004 non-null  object\n",
            " 2   Computer Science                              14004 non-null  int64 \n",
            " 3   Mathematics                                   14004 non-null  int64 \n",
            " 4   Physics                                       14004 non-null  int64 \n",
            " 5   Statistics                                    14004 non-null  int64 \n",
            " 6   Analysis of PDEs                              14004 non-null  int64 \n",
            " 7   Applications                                  14004 non-null  int64 \n",
            " 8   Artificial Intelligence                       14004 non-null  int64 \n",
            " 9   Astrophysics of Galaxies                      14004 non-null  int64 \n",
            " 10  Computation and Language                      14004 non-null  int64 \n",
            " 11  Computer Vision and Pattern Recognition       14004 non-null  int64 \n",
            " 12  Cosmology and Nongalactic Astrophysics        14004 non-null  int64 \n",
            " 13  Data Structures and Algorithms                14004 non-null  int64 \n",
            " 14  Differential Geometry                         14004 non-null  int64 \n",
            " 15  Earth and Planetary Astrophysics              14004 non-null  int64 \n",
            " 16  Fluid Dynamics                                14004 non-null  int64 \n",
            " 17  Information Theory                            14004 non-null  int64 \n",
            " 18  Instrumentation and Methods for Astrophysics  14004 non-null  int64 \n",
            " 19  Machine Learning                              14004 non-null  int64 \n",
            " 20  Materials Science                             14004 non-null  int64 \n",
            " 21  Methodology                                   14004 non-null  int64 \n",
            " 22  Number Theory                                 14004 non-null  int64 \n",
            " 23  Optimization and Control                      14004 non-null  int64 \n",
            " 24  Representation Theory                         14004 non-null  int64 \n",
            " 25  Robotics                                      14004 non-null  int64 \n",
            " 26  Social and Information Networks               14004 non-null  int64 \n",
            " 27  Statistics Theory                             14004 non-null  int64 \n",
            " 28  Strongly Correlated Electrons                 14004 non-null  int64 \n",
            " 29  Superconductivity                             14004 non-null  int64 \n",
            " 30  Systems and Control                           14004 non-null  int64 \n",
            "dtypes: int64(30), object(1)\n",
            "memory usage: 3.3+ MB\n"
          ]
        }
      ],
      "source": [
        "#Display all the columns of the train dataset\n",
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGUIO3IIBA2D"
      },
      "outputs": [],
      "source": [
        "#We divided the columns into TARGET_COLS and TOPIC_COLS  \n",
        "ID_COL = 'id'\n",
        "\n",
        "TARGET_COLS = ['Analysis of PDEs', 'Applications',\n",
        "               'Artificial Intelligence', 'Astrophysics of Galaxies',\n",
        "               'Computation and Language', 'Computer Vision and Pattern Recognition',\n",
        "               'Cosmology and Nongalactic Astrophysics',\n",
        "               'Data Structures and Algorithms', 'Differential Geometry',\n",
        "               'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n",
        "               'Information Theory', 'Instrumentation and Methods for Astrophysics',\n",
        "               'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n",
        "               'Optimization and Control', 'Representation Theory', 'Robotics',\n",
        "               'Social and Information Networks', 'Statistics Theory',\n",
        "               'Strongly Correlated Electrons', 'Superconductivity',\n",
        "               'Systems and Control']\n",
        "#The research article abstracts are sourced from the following 4 topics\n",
        "TOPIC_COLS = ['Computer Science', 'Mathematics', 'Physics', 'Statistics']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYbM4PFFBAwm",
        "outputId": "6ac11f42-87f0-44ee-a074-6526a4f7e5ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Computer Science    5884\n",
              "Physics             3856\n",
              "Statistics          3794\n",
              "Mathematics         2831\n",
              "dtype: int64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Sorting in descending order according to values corresponding to topics \n",
        "train[TOPIC_COLS].sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH1tF8UMBAtl",
        "outputId": "7468e324-3898-47ae-f697-389bb37b487f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Machine Learning                                27.313625\n",
              "Artificial Intelligence                          9.825764\n",
              "Robotics                                         6.812339\n",
              "Computer Vision and Pattern Recognition          6.705227\n",
              "Strongly Correlated Electrons                    6.376750\n",
              "Materials Science                                5.534133\n",
              "Computation and Language                         4.605827\n",
              "Cosmology and Nongalactic Astrophysics           4.520137\n",
              "Optimization and Control                         4.434447\n",
              "Social and Information Networks                  4.420166\n",
              "Analysis of PDEs                                 4.334476\n",
              "Applications                                     4.227364\n",
              "Astrophysics of Galaxies                         4.098829\n",
              "Methodology                                      4.098829\n",
              "Systems and Control                              4.041702\n",
              "Differential Geometry                            3.963153\n",
              "Superconductivity                                3.870323\n",
              "Statistics Theory                                3.791774\n",
              "Earth and Planetary Astrophysics                 3.598972\n",
              "Data Structures and Algorithms                   3.541845\n",
              "Instrumentation and Methods for Astrophysics     3.499000\n",
              "Number Theory                                    2.842045\n",
              "Representation Theory                            2.613539\n",
              "Fluid Dynamics                                   2.599257\n",
              "Information Theory                               2.520708\n",
              "dtype: float64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "100 * (train[TARGET_COLS].sum()/(train.shape[0])).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GduxY7dBf0x"
      },
      "source": [
        "## **1] Using Count Vectorizer** \n",
        "\n",
        "It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. It creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md4aqgpPBAoi",
        "outputId": "d78c21f4-f681-43c4-9b63-7dabb0d6e5de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CountVectorizer(max_features=10000)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec = CountVectorizer(max_features=10000)\n",
        "combined = list(train['ABSTRACT']) + list(test['ABSTRACT'])\n",
        "vec.fit(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5qY9mjMBAlv"
      },
      "outputs": [],
      "source": [
        "trn, val = train_test_split(train, test_size=0.2, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg7dSX4_BAhB",
        "outputId": "e6506903-0f58-4a9a-ab40-8002e2238a2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<11203x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1033404 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_abs = vec.transform(trn['ABSTRACT'])\n",
        "val_abs = vec.transform(val['ABSTRACT'])\n",
        "tst_abs = vec.transform(test['ABSTRACT'])\n",
        "\n",
        "trn_abs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d72bgy6iBAYw",
        "outputId": "6f328966-f903-4833-951c-6a962fb97eee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['00',\n",
              " '000',\n",
              " '0001',\n",
              " '001',\n",
              " '005',\n",
              " '01',\n",
              " '02',\n",
              " '03',\n",
              " '04',\n",
              " '05',\n",
              " '06',\n",
              " '07',\n",
              " '08',\n",
              " '09',\n",
              " '10',\n",
              " '100',\n",
              " '1000',\n",
              " '101',\n",
              " '102',\n",
              " '10x',\n",
              " '11',\n",
              " '110',\n",
              " '111',\n",
              " '112',\n",
              " '115',\n",
              " '118',\n",
              " '12',\n",
              " '120',\n",
              " '128',\n",
              " '13',\n",
              " '130',\n",
              " '14',\n",
              " '140',\n",
              " '15',\n",
              " '150',\n",
              " '1500',\n",
              " '16',\n",
              " '160',\n",
              " '17',\n",
              " '170',\n",
              " '18',\n",
              " '180',\n",
              " '19',\n",
              " '1977',\n",
              " '1986',\n",
              " '1991',\n",
              " '1994',\n",
              " '1995',\n",
              " '1996',\n",
              " '1997',\n",
              " '1998',\n",
              " '1999',\n",
              " '1d',\n",
              " '1t',\n",
              " '20',\n",
              " '200',\n",
              " '2000',\n",
              " '2001',\n",
              " '2002',\n",
              " '2003',\n",
              " '2004',\n",
              " '2005',\n",
              " '2006',\n",
              " '2007',\n",
              " '2008',\n",
              " '2009',\n",
              " '2010',\n",
              " '2011',\n",
              " '2012',\n",
              " '2013',\n",
              " '2014',\n",
              " '2015',\n",
              " '2016',\n",
              " '2017',\n",
              " '2018',\n",
              " '21',\n",
              " '22',\n",
              " '23',\n",
              " '230',\n",
              " '24',\n",
              " '240',\n",
              " '25',\n",
              " '250',\n",
              " '2500',\n",
              " '256',\n",
              " '26',\n",
              " '2600',\n",
              " '27',\n",
              " '28',\n",
              " '29',\n",
              " '2_',\n",
              " '2d',\n",
              " '2g',\n",
              " '2h',\n",
              " '2k',\n",
              " '2m',\n",
              " '2mass',\n",
              " '2n',\n",
              " '2p',\n",
              " '2s',\n",
              " '2x',\n",
              " '30',\n",
              " '300',\n",
              " '3000',\n",
              " '31',\n",
              " '32',\n",
              " '33',\n",
              " '34',\n",
              " '35',\n",
              " '350',\n",
              " '36',\n",
              " '360',\n",
              " '37',\n",
              " '38',\n",
              " '39',\n",
              " '3b',\n",
              " '3d',\n",
              " '3x',\n",
              " '40',\n",
              " '400',\n",
              " '4000',\n",
              " '41',\n",
              " '42',\n",
              " '43',\n",
              " '44',\n",
              " '45',\n",
              " '46',\n",
              " '47',\n",
              " '48',\n",
              " '49',\n",
              " '4d',\n",
              " '4f',\n",
              " '50',\n",
              " '500',\n",
              " '5000',\n",
              " '51',\n",
              " '52',\n",
              " '53',\n",
              " '54',\n",
              " '55',\n",
              " '56',\n",
              " '57',\n",
              " '58',\n",
              " '59',\n",
              " '5d',\n",
              " '5x',\n",
              " '60',\n",
              " '600',\n",
              " '61',\n",
              " '62',\n",
              " '63',\n",
              " '64',\n",
              " '65',\n",
              " '66',\n",
              " '67',\n",
              " '67p',\n",
              " '68',\n",
              " '69',\n",
              " '6d',\n",
              " '6x',\n",
              " '70',\n",
              " '700',\n",
              " '71',\n",
              " '72',\n",
              " '73',\n",
              " '74',\n",
              " '75',\n",
              " '76',\n",
              " '77',\n",
              " '78',\n",
              " '79',\n",
              " '80',\n",
              " '800',\n",
              " '81',\n",
              " '82',\n",
              " '83',\n",
              " '84',\n",
              " '85',\n",
              " '86',\n",
              " '87',\n",
              " '88',\n",
              " '89',\n",
              " '90',\n",
              " '91',\n",
              " '92',\n",
              " '93',\n",
              " '94',\n",
              " '95',\n",
              " '96',\n",
              " '97',\n",
              " '98',\n",
              " '99',\n",
              " '_0',\n",
              " '_1',\n",
              " '_2',\n",
              " '_3',\n",
              " '_4',\n",
              " '_5',\n",
              " '_6',\n",
              " '_7',\n",
              " '_c',\n",
              " '_e',\n",
              " '_f',\n",
              " '_i',\n",
              " '_j',\n",
              " '_k',\n",
              " '_l',\n",
              " '_m',\n",
              " '_n',\n",
              " '_p',\n",
              " '_q',\n",
              " '_x',\n",
              " '_y',\n",
              " 'a3c',\n",
              " 'a_',\n",
              " 'a_1',\n",
              " 'a_i',\n",
              " 'a_n',\n",
              " 'aa',\n",
              " 'ab',\n",
              " 'abc',\n",
              " 'abelian',\n",
              " 'abell',\n",
              " 'aberrations',\n",
              " 'abilities',\n",
              " 'ability',\n",
              " 'ablation',\n",
              " 'able',\n",
              " 'abnormal',\n",
              " 'abnormalities',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abridged',\n",
              " 'abrupt',\n",
              " 'absence',\n",
              " 'absent',\n",
              " 'absolute',\n",
              " 'absolutely',\n",
              " 'absorbed',\n",
              " 'absorber',\n",
              " 'absorbers',\n",
              " 'absorbing',\n",
              " 'absorption',\n",
              " 'abstract',\n",
              " 'abstraction',\n",
              " 'abstractions',\n",
              " 'abstracts',\n",
              " 'abundance',\n",
              " 'abundances',\n",
              " 'abundant',\n",
              " 'ac',\n",
              " 'academic',\n",
              " 'accelerate',\n",
              " 'accelerated',\n",
              " 'accelerates',\n",
              " 'accelerating',\n",
              " 'acceleration',\n",
              " 'accelerations',\n",
              " 'accelerator',\n",
              " 'accept',\n",
              " 'acceptable',\n",
              " 'acceptance',\n",
              " 'accepted',\n",
              " 'access',\n",
              " 'accessed',\n",
              " 'accesses',\n",
              " 'accessibility',\n",
              " 'accessible',\n",
              " 'accessing',\n",
              " 'accident',\n",
              " 'accidental',\n",
              " 'accidents',\n",
              " 'accommodate',\n",
              " 'accommodated',\n",
              " 'accommodates',\n",
              " 'accompanied',\n",
              " 'accompanying',\n",
              " 'accomplish',\n",
              " 'accomplished',\n",
              " 'accordance',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'account',\n",
              " 'accounted',\n",
              " 'accounting',\n",
              " 'accounts',\n",
              " 'accrete',\n",
              " 'accreted',\n",
              " 'accreting',\n",
              " 'accretion',\n",
              " 'accumulate',\n",
              " 'accumulated',\n",
              " 'accumulation',\n",
              " 'accuracies',\n",
              " 'accuracy',\n",
              " 'accurate',\n",
              " 'accurately',\n",
              " 'achievable',\n",
              " 'achieve',\n",
              " 'achieved',\n",
              " 'achievement',\n",
              " 'achievements',\n",
              " 'achieves',\n",
              " 'achieving',\n",
              " 'acid',\n",
              " 'acoustic',\n",
              " 'acquire',\n",
              " 'acquired',\n",
              " 'acquires',\n",
              " 'acquiring',\n",
              " 'acquisition',\n",
              " 'across',\n",
              " 'acs',\n",
              " 'act',\n",
              " 'acting',\n",
              " 'action',\n",
              " 'actions',\n",
              " 'activated',\n",
              " 'activation',\n",
              " 'activations',\n",
              " 'active',\n",
              " 'actively',\n",
              " 'activities',\n",
              " 'activity',\n",
              " 'actor',\n",
              " 'actors',\n",
              " 'acts',\n",
              " 'actual',\n",
              " 'actually',\n",
              " 'actuated',\n",
              " 'actuation',\n",
              " 'actuator',\n",
              " 'actuators',\n",
              " 'acute',\n",
              " 'acyclic',\n",
              " 'ad',\n",
              " 'adam',\n",
              " 'adapt',\n",
              " 'adaptation',\n",
              " 'adapted',\n",
              " 'adapting',\n",
              " 'adaptive',\n",
              " 'adaptively',\n",
              " 'adaptivity',\n",
              " 'adapts',\n",
              " 'adc',\n",
              " 'add',\n",
              " 'added',\n",
              " 'adding',\n",
              " 'addition',\n",
              " 'additional',\n",
              " 'additionally',\n",
              " 'additive',\n",
              " 'address',\n",
              " 'addressed',\n",
              " 'addresses',\n",
              " 'addressing',\n",
              " 'adds',\n",
              " 'adequate',\n",
              " 'adequately',\n",
              " 'adiabatic',\n",
              " 'adic',\n",
              " 'adjacency',\n",
              " 'adjacent',\n",
              " 'adjoint',\n",
              " 'adjust',\n",
              " 'adjusted',\n",
              " 'adjusting',\n",
              " 'adjustment',\n",
              " 'adjusts',\n",
              " 'admissible',\n",
              " 'admit',\n",
              " 'admits',\n",
              " 'admitting',\n",
              " 'admm',\n",
              " 'adopt',\n",
              " 'adopted',\n",
              " 'adopting',\n",
              " 'adoption',\n",
              " 'adopts',\n",
              " 'ads',\n",
              " 'adsorption',\n",
              " 'advance',\n",
              " 'advanced',\n",
              " 'advancement',\n",
              " 'advancements',\n",
              " 'advances',\n",
              " 'advancing',\n",
              " 'advantage',\n",
              " 'advantageous',\n",
              " 'advantages',\n",
              " 'advection',\n",
              " 'advent',\n",
              " 'adversarial',\n",
              " 'adversarially',\n",
              " 'adversaries',\n",
              " 'adversary',\n",
              " 'adverse',\n",
              " 'advertisement',\n",
              " 'advertisements',\n",
              " 'advertising',\n",
              " 'advice',\n",
              " 'advocate',\n",
              " 'ae',\n",
              " 'aerial',\n",
              " 'aerosol',\n",
              " 'aes',\n",
              " 'aesthetic',\n",
              " 'af',\n",
              " 'affect',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affective',\n",
              " 'affects',\n",
              " 'affine',\n",
              " 'affinity',\n",
              " 'affordable',\n",
              " 'affordance',\n",
              " 'affordances',\n",
              " 'afm',\n",
              " 'aforementioned',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'ag',\n",
              " 'again',\n",
              " 'against',\n",
              " 'agb',\n",
              " 'age',\n",
              " 'aged',\n",
              " 'agency',\n",
              " 'agent',\n",
              " 'agents',\n",
              " 'ages',\n",
              " 'aggregate',\n",
              " 'aggregated',\n",
              " 'aggregates',\n",
              " 'aggregating',\n",
              " 'aggregation',\n",
              " 'aggregator',\n",
              " 'aggressive',\n",
              " 'agile',\n",
              " 'aging',\n",
              " 'agn',\n",
              " 'agnostic',\n",
              " 'agns',\n",
              " 'ago',\n",
              " 'agree',\n",
              " 'agreed',\n",
              " 'agreement',\n",
              " 'agrees',\n",
              " 'ahead',\n",
              " 'ai',\n",
              " 'aic',\n",
              " 'aid',\n",
              " 'aided',\n",
              " 'aim',\n",
              " 'aimed',\n",
              " 'aiming',\n",
              " 'aims',\n",
              " 'air',\n",
              " 'aircraft',\n",
              " 'aka',\n",
              " 'akin',\n",
              " 'al',\n",
              " 'alarm',\n",
              " 'alarms',\n",
              " 'albedo',\n",
              " 'albeit',\n",
              " 'alexnet',\n",
              " 'algebra',\n",
              " 'algebraic',\n",
              " 'algebraically',\n",
              " 'algebras',\n",
              " 'algebroid',\n",
              " 'algorithm',\n",
              " 'algorithmic',\n",
              " 'algorithmically',\n",
              " 'algorithms',\n",
              " 'aliasing',\n",
              " 'alice',\n",
              " 'align',\n",
              " 'aligned',\n",
              " 'aligning',\n",
              " 'alignment',\n",
              " 'alignments',\n",
              " 'alkali',\n",
              " 'all',\n",
              " 'allen',\n",
              " 'alleviate',\n",
              " 'alleviates',\n",
              " 'alleviating',\n",
              " 'allocated',\n",
              " 'allocation',\n",
              " 'allotropes',\n",
              " 'allow',\n",
              " 'allowed',\n",
              " 'allowing',\n",
              " 'allows',\n",
              " 'alloy',\n",
              " 'alloys',\n",
              " 'alma',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'alpha',\n",
              " 'alpha_',\n",
              " 'alphabet',\n",
              " 'already',\n",
              " 'also',\n",
              " 'alter',\n",
              " 'altered',\n",
              " 'altering',\n",
              " 'alternate',\n",
              " 'alternating',\n",
              " 'alternative',\n",
              " 'alternatively',\n",
              " 'alternatives',\n",
              " 'alters',\n",
              " 'although',\n",
              " 'altitude',\n",
              " 'altogether',\n",
              " 'always',\n",
              " 'alzheimer',\n",
              " 'am',\n",
              " 'amazon',\n",
              " 'ambient',\n",
              " 'ambiguities',\n",
              " 'ambiguity',\n",
              " 'ambiguous',\n",
              " 'amenable',\n",
              " 'american',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amorphous',\n",
              " 'amortized',\n",
              " 'amount',\n",
              " 'amounts',\n",
              " 'amp',\n",
              " 'amplification',\n",
              " 'amplified',\n",
              " 'amplify',\n",
              " 'amplitude',\n",
              " 'amplitudes',\n",
              " 'ampère',\n",
              " 'an',\n",
              " 'anal',\n",
              " 'analog',\n",
              " 'analogies',\n",
              " 'analogous',\n",
              " 'analogs',\n",
              " 'analogue',\n",
              " 'analogues',\n",
              " 'analogy',\n",
              " 'analyse',\n",
              " 'analysed',\n",
              " 'analyses',\n",
              " 'analysing',\n",
              " 'analysis',\n",
              " 'analyst',\n",
              " 'analysts',\n",
              " 'analytic',\n",
              " 'analytical',\n",
              " 'analytically',\n",
              " 'analyticity',\n",
              " 'analytics',\n",
              " 'analyze',\n",
              " 'analyzed',\n",
              " 'analyzes',\n",
              " 'analyzing',\n",
              " 'anatomical',\n",
              " 'anchor',\n",
              " 'ancient',\n",
              " 'ancillary',\n",
              " 'and',\n",
              " 'anderson',\n",
              " 'andreev',\n",
              " 'android',\n",
              " 'angle',\n",
              " 'angles',\n",
              " 'angular',\n",
              " 'animal',\n",
              " 'animals',\n",
              " 'anisotropic',\n",
              " 'anisotropies',\n",
              " 'anisotropy',\n",
              " 'ann',\n",
              " 'annealing',\n",
              " 'annihilation',\n",
              " 'annotate',\n",
              " 'annotated',\n",
              " 'annotating',\n",
              " 'annotation',\n",
              " 'annotations',\n",
              " 'annotator',\n",
              " 'annotators',\n",
              " 'anns',\n",
              " 'annual',\n",
              " 'anomalies',\n",
              " 'anomalous',\n",
              " 'anomalously',\n",
              " 'anomaly',\n",
              " 'anonymization',\n",
              " 'anonymous',\n",
              " 'anosov',\n",
              " 'another',\n",
              " 'ansatz',\n",
              " 'answer',\n",
              " 'answered',\n",
              " 'answering',\n",
              " 'answers',\n",
              " 'ant',\n",
              " 'antenna',\n",
              " 'antennas',\n",
              " 'anti',\n",
              " 'anticipate',\n",
              " 'anticipated',\n",
              " 'antiferromagnet',\n",
              " 'antiferromagnetic',\n",
              " 'antiferromagnetically',\n",
              " 'antiferromagnetism',\n",
              " 'antiferromagnets',\n",
              " 'any',\n",
              " 'anyons',\n",
              " 'anytime',\n",
              " 'anywhere',\n",
              " 'ao',\n",
              " 'aod',\n",
              " 'aoi',\n",
              " 'ap',\n",
              " 'apache',\n",
              " 'apart',\n",
              " 'aperture',\n",
              " 'api',\n",
              " 'app',\n",
              " 'apparent',\n",
              " 'apparently',\n",
              " 'appealing',\n",
              " 'appear',\n",
              " 'appearance',\n",
              " 'appearances',\n",
              " 'appeared',\n",
              " 'appearing',\n",
              " 'appears',\n",
              " 'appendix',\n",
              " 'appl',\n",
              " 'applicability',\n",
              " 'applicable',\n",
              " 'applicants',\n",
              " 'application',\n",
              " 'applications',\n",
              " 'applied',\n",
              " 'applies',\n",
              " 'apply',\n",
              " 'applying',\n",
              " 'appreciable',\n",
              " 'approach',\n",
              " 'approached',\n",
              " 'approaches',\n",
              " 'approaching',\n",
              " 'appropriate',\n",
              " 'appropriately',\n",
              " 'approx',\n",
              " 'approximate',\n",
              " 'approximated',\n",
              " 'approximately',\n",
              " 'approximates',\n",
              " 'approximating',\n",
              " 'approximation',\n",
              " 'approximations',\n",
              " 'apps',\n",
              " 'april',\n",
              " 'aps',\n",
              " 'aqueous',\n",
              " 'ar',\n",
              " 'arabic',\n",
              " 'arbitrarily',\n",
              " 'arbitrary',\n",
              " 'arc',\n",
              " 'archimedean',\n",
              " 'architectural',\n",
              " 'architecture',\n",
              " 'architectures',\n",
              " 'archival',\n",
              " 'archive',\n",
              " 'archives',\n",
              " 'arcmin',\n",
              " 'arcs',\n",
              " 'arcsec',\n",
              " 'arcsecond',\n",
              " 'are',\n",
              " 'area',\n",
              " 'areas',\n",
              " 'argon',\n",
              " 'arguably',\n",
              " 'argue',\n",
              " 'argued',\n",
              " 'argues',\n",
              " 'arguing',\n",
              " 'argument',\n",
              " 'argumentation',\n",
              " 'arguments',\n",
              " 'arise',\n",
              " 'arises',\n",
              " 'arising',\n",
              " 'arithmetic',\n",
              " 'arm',\n",
              " 'armchair',\n",
              " 'armed',\n",
              " 'arms',\n",
              " 'around',\n",
              " 'arpes',\n",
              " 'arranged',\n",
              " 'arrangement',\n",
              " 'arrangements',\n",
              " 'array',\n",
              " 'arrays',\n",
              " 'arrival',\n",
              " 'arrivals',\n",
              " 'arrive',\n",
              " 'arriving',\n",
              " 'art',\n",
              " 'artefacts',\n",
              " 'artery',\n",
              " 'article',\n",
              " 'articles',\n",
              " 'articulated',\n",
              " 'artifact',\n",
              " 'artifacts',\n",
              " 'artificial',\n",
              " 'artificially',\n",
              " 'artin',\n",
              " 'arts',\n",
              " 'arxiv',\n",
              " 'as',\n",
              " 'ascent',\n",
              " 'ascribed',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asking',\n",
              " 'asks',\n",
              " 'asp',\n",
              " 'aspect',\n",
              " 'aspects',\n",
              " 'asr',\n",
              " 'assembled',\n",
              " 'assembly',\n",
              " 'assess',\n",
              " 'assessed',\n",
              " 'assessing',\n",
              " 'assessment',\n",
              " 'assessments',\n",
              " 'asset',\n",
              " 'assets',\n",
              " 'assign',\n",
              " 'assigned',\n",
              " 'assigning',\n",
              " 'assignment',\n",
              " 'assignments',\n",
              " 'assigns',\n",
              " 'assimilation',\n",
              " 'assist',\n",
              " 'assistance',\n",
              " 'assistant',\n",
              " 'assisted',\n",
              " 'assistive',\n",
              " 'associate',\n",
              " 'associated',\n",
              " 'associating',\n",
              " 'association',\n",
              " 'associations',\n",
              " 'associative',\n",
              " 'assortativity',\n",
              " 'assume',\n",
              " 'assumed',\n",
              " 'assumes',\n",
              " 'assuming',\n",
              " 'assumption',\n",
              " 'assumptions',\n",
              " 'assurance',\n",
              " 'ast',\n",
              " 'asteroid',\n",
              " 'asteroids',\n",
              " 'astrometric',\n",
              " 'astrometry',\n",
              " 'astronomers',\n",
              " 'astronomical',\n",
              " 'astronomy',\n",
              " 'astrophysical',\n",
              " 'astrophysics',\n",
              " 'asymmetric',\n",
              " 'asymmetries',\n",
              " 'asymmetry',\n",
              " 'asymptotic',\n",
              " 'asymptotically',\n",
              " 'asymptotics',\n",
              " 'asynchronous',\n",
              " 'at',\n",
              " 'atacama',\n",
              " 'atari',\n",
              " 'atlas',\n",
              " 'atmosphere',\n",
              " 'atmospheres',\n",
              " 'atmospheric',\n",
              " 'atom',\n",
              " 'atomic',\n",
              " 'atomically',\n",
              " 'atomistic',\n",
              " 'atoms',\n",
              " 'attached',\n",
              " 'attachment',\n",
              " 'attack',\n",
              " 'attacker',\n",
              " 'attackers',\n",
              " 'attacks',\n",
              " 'attain',\n",
              " 'attainable',\n",
              " 'attained',\n",
              " 'attaining',\n",
              " 'attains',\n",
              " 'attempt',\n",
              " 'attempted',\n",
              " 'attempting',\n",
              " 'attempts',\n",
              " 'attend',\n",
              " 'attention',\n",
              " 'attentional',\n",
              " 'attentions',\n",
              " 'attentive',\n",
              " 'attenuation',\n",
              " 'attitude',\n",
              " 'attitudes',\n",
              " 'attract',\n",
              " 'attracted',\n",
              " 'attracting',\n",
              " 'attraction',\n",
              " 'attractive',\n",
              " 'attractor',\n",
              " 'attractors',\n",
              " 'attribute',\n",
              " 'attributed',\n",
              " 'attributes',\n",
              " 'attribution',\n",
              " 'au',\n",
              " 'auc',\n",
              " 'auction',\n",
              " 'auctions',\n",
              " 'audience',\n",
              " 'audio',\n",
              " 'audit',\n",
              " 'audits',\n",
              " 'augment',\n",
              " 'augmentation',\n",
              " 'augmented',\n",
              " 'augmenting',\n",
              " 'augments',\n",
              " 'auslander',\n",
              " 'authentication',\n",
              " 'author',\n",
              " 'authority',\n",
              " 'authors',\n",
              " 'authorship',\n",
              " 'auto',\n",
              " 'autocorrelation',\n",
              " 'autoencoder',\n",
              " 'autoencoders',\n",
              " 'automata',\n",
              " 'automate',\n",
              " 'automated',\n",
              " 'automatic',\n",
              " 'automatically',\n",
              " 'automating',\n",
              " 'automation',\n",
              " 'automaton',\n",
              " 'automl',\n",
              " 'automorphic',\n",
              " 'automorphism',\n",
              " 'automorphisms',\n",
              " 'automotive',\n",
              " 'autonomous',\n",
              " 'autonomously',\n",
              " 'autonomy',\n",
              " 'autoregressive',\n",
              " 'auxiliary',\n",
              " 'availability',\n",
              " 'available',\n",
              " 'avalanche',\n",
              " 'avalanches',\n",
              " 'avenue',\n",
              " 'avenues',\n",
              " 'average',\n",
              " 'averaged',\n",
              " 'averages',\n",
              " 'averaging',\n",
              " 'averse',\n",
              " 'avoid',\n",
              " 'avoidance',\n",
              " 'avoided',\n",
              " 'avoiding',\n",
              " 'avoids',\n",
              " 'avs',\n",
              " 'aware',\n",
              " 'awareness',\n",
              " 'away',\n",
              " 'ax',\n",
              " 'axes',\n",
              " 'axial',\n",
              " 'axiomatic',\n",
              " 'axioms',\n",
              " 'axion',\n",
              " 'axis',\n",
              " 'axisymmetric',\n",
              " 'azimuthal',\n",
              " 'b_',\n",
              " 'b_1',\n",
              " 'ba',\n",
              " 'babi',\n",
              " 'back',\n",
              " 'backbone',\n",
              " 'background',\n",
              " 'backgrounds',\n",
              " 'backpropagation',\n",
              " 'backward',\n",
              " 'bad',\n",
              " 'bafe',\n",
              " 'bag',\n",
              " 'bagging',\n",
              " 'bags',\n",
              " 'bal',\n",
              " 'balance',\n",
              " 'balanced',\n",
              " 'balances',\n",
              " 'balancing',\n",
              " 'ball',\n",
              " 'ballistic',\n",
              " 'balloon',\n",
              " 'balls',\n",
              " 'banach',\n",
              " 'band',\n",
              " 'bandgap',\n",
              " 'bandit',\n",
              " 'bandits',\n",
              " 'bands',\n",
              " 'bandwidth',\n",
              " 'bang',\n",
              " 'bank',\n",
              " 'bao',\n",
              " 'bar',\n",
              " 'bardeen',\n",
              " 'bare',\n",
              " 'barocaloric',\n",
              " 'barrier',\n",
              " 'barriers',\n",
              " 'bars',\n",
              " 'barycenter',\n",
              " 'baryon',\n",
              " 'baryonic',\n",
              " 'baryons',\n",
              " 'base',\n",
              " 'based',\n",
              " 'baseline',\n",
              " 'baselines',\n",
              " 'bases',\n",
              " 'basic',\n",
              " 'basically',\n",
              " 'basin',\n",
              " 'basis',\n",
              " 'basketball',\n",
              " 'batch',\n",
              " 'batches',\n",
              " 'bath',\n",
              " 'batteries',\n",
              " 'battery',\n",
              " 'baxter',\n",
              " 'bayes',\n",
              " 'bayesian',\n",
              " 'bb',\n",
              " 'bbb',\n",
              " 'bc',\n",
              " 'bcc',\n",
              " 'bci',\n",
              " 'bcs',\n",
              " 'be',\n",
              " 'beacon',\n",
              " 'beam',\n",
              " 'beamforming',\n",
              " 'beaming',\n",
              " 'beams',\n",
              " 'bear',\n",
              " 'bearing',\n",
              " 'beat',\n",
              " 'beats',\n",
              " 'bec',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'bed',\n",
              " 'bedt',\n",
              " ...]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec.get_feature_names() # these are our vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKJrBljeB3KX",
        "outputId": "7e62904e-faa7-4b38-e530-42eb5b6ca9a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LogisticRegression(C=10, n_jobs=-1))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = OneVsRestClassifier(LogisticRegression(C = 10, n_jobs=-1))\n",
        "clf.fit(trn_abs, trn[TARGET_COLS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOPAbOgdB3GQ",
        "outputId": "2579a96a-7c67-401f-e3ee-091bec735645"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.623196768609348"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_preds = clf.predict(val_abs)\n",
        "f1_score(val[TARGET_COLS], val_preds, average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glaC6ELGB_z4"
      },
      "source": [
        "**2] Using Tfidf Vectorizer** - Term frequency-inverse document frequency is a text vectorizer that transforms the text into a usable vector.\n",
        "\n",
        "\n",
        "*    Term frequency represents every text from the data as a matrix whose rows are the number of documents and columns are the number of distinct terms throughout all documents.\n",
        "*   Inverse document frequency (IDF) is the weight of a term, it aims to reduce the weight of a term if the term’s occurrences are scattered throughout all the documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFed2TODB3Bn",
        "outputId": "454e67fc-eb3c-4d37-b588-df7482c26382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6300845429893125"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec = TfidfVectorizer(max_features=10000)\n",
        "_ = vec.fit(list(train['ABSTRACT']) + list(test['ABSTRACT']))\n",
        "\n",
        "trn_abs = vec.transform(trn['ABSTRACT'])\n",
        "val_abs = vec.transform(val['ABSTRACT'])\n",
        "tst_abs = vec.transform(test['ABSTRACT'])\n",
        "\n",
        "clf = OneVsRestClassifier(LogisticRegression(C = 10, n_jobs=-1))\n",
        "_ = clf.fit(trn_abs, trn[TARGET_COLS])\n",
        "\n",
        "val_preds = clf.predict(val_abs)\n",
        "f1_score(val[TARGET_COLS], val_preds, average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPVjBZE_GIGj"
      },
      "source": [
        "## **Sequential Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKKgNlVwGPTG"
      },
      "source": [
        "### Building Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghmf4qymB29Y",
        "outputId": "97cd628d-81ff-457a-8279-b0db2d73280c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['id', 'ABSTRACT', 'Computer Science', 'Mathematics', 'Physics',\n",
              "       'Statistics', 'Analysis of PDEs', 'Applications',\n",
              "       'Artificial Intelligence', 'Astrophysics of Galaxies',\n",
              "       'Computation and Language', 'Computer Vision and Pattern Recognition',\n",
              "       'Cosmology and Nongalactic Astrophysics',\n",
              "       'Data Structures and Algorithms', 'Differential Geometry',\n",
              "       'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n",
              "       'Information Theory', 'Instrumentation and Methods for Astrophysics',\n",
              "       'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n",
              "       'Optimization and Control', 'Representation Theory', 'Robotics',\n",
              "       'Social and Information Networks', 'Statistics Theory',\n",
              "       'Strongly Correlated Electrons', 'Superconductivity',\n",
              "       'Systems and Control', 'text'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['text'] = ' '\n",
        "test['text'] = ' '\n",
        "\n",
        "#this is our corpus basically\n",
        "train['text'] += train['ABSTRACT'] \n",
        "test['text'] += test['ABSTRACT']\n",
        "\n",
        "#Splitting the training data into test and train dataset where 20% is used for testing and 80% for training\n",
        "trn, val = train_test_split(train, test_size=0.2, random_state=2)\n",
        "\n",
        "trn.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4MYmKpPGWj8"
      },
      "source": [
        "### Tokenizing the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGll1FJBB24o",
        "outputId": "526ca1a7-b5b4-419f-9190-1e9cff652f81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "51665"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#100000 is the max. no. of words to keep in the tokenized list\n",
        "tok = Tokenizer(num_words = 1000000) \n",
        "tok.fit_on_texts(train['text'].str.lower().tolist() + test['text'].str.lower().tolist()) #updates the internal dictionary based on a list of words\n",
        "\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxecWgPeGoZF"
      },
      "source": [
        "### Converting into column vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv4h-9akB2zg"
      },
      "outputs": [],
      "source": [
        "# For each word in each sentences, converting into column vector and producing the one hot encoding\n",
        "X_trn = tok.texts_to_sequences(trn['text'])\n",
        "X_val = tok.texts_to_sequences(val['text'])\n",
        "X_test = tok.texts_to_sequences(test['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPWE0RSSB2uh",
        "outputId": "1eaaafdc-36f0-4749-f6ee-a8f139d20ad0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,   280,   965,    53],\n",
              "       [    0,     0,     0, ...,  1278,   423,  4957],\n",
              "       [    1, 10832,    75, ...,     5,  9884,  4154],\n",
              "       ...,\n",
              "       [   36,  1514,    10, ...,    99,   264,  2804],\n",
              "       [    2,  7933,    22, ...,    62,   123,   125],\n",
              "       [    0,     0,     0, ...,   412,  6056,   164]], dtype=int32)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "maxlen = 200 #maximum length of all sequences(i.e, to what length is each sentence filled upto)\n",
        "X_trn = pad_sequences(X_trn, maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP4MDmu4GyZi"
      },
      "source": [
        "### Creating a **Word Embedding Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yampF9EcB2nc",
        "outputId": "db561bef-0ffb-4917-9eac-c30ae3931b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 50)           2583250   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 10000)             0         \n",
            "                                                                 \n",
            " Fully_Connected (Dense)     (None, 200)               2000200   \n",
            "                                                                 \n",
            " Output (Dense)              (None, 25)                5025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,588,475\n",
            "Trainable params: 4,588,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, SpatialDropout1D, LSTM\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "embedding_dim = 50 # taken 50 'features'\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "\n",
        "# Allow algorithms to distinguish context and figure out what words are related to each other\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    input_length=maxlen))\n",
        "\n",
        "# Using back propagation method, our model will start learning\n",
        "model.add(Flatten()) # This will convert the embedding matrix into a single column vector \n",
        "model.add(Dense(200, activation='relu', name = 'Fully_Connected'))\n",
        "model.add(Dense(25, activation='sigmoid', name = 'Output'))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-3),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['categorical_accuracy'],\n",
        "              )\n",
        "\n",
        "model.summary() # The model is created and this is the summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GryRU6khG-h5"
      },
      "source": [
        "### Fitting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgk4iZ8UG5s8",
        "outputId": "5353facd-4386-44cd-ab06-f4da4cb6a013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f4e9dc7edd0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f4e9dc7edd0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "44/44 [==============================] - ETA: 0s - loss: 0.2799 - categorical_accuracy: 0.1451WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f4e9d77c560> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f4e9d77c560> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "44/44 [==============================] - 7s 123ms/step - loss: 0.2799 - categorical_accuracy: 0.1451 - val_loss: 0.1934 - val_categorical_accuracy: 0.1649 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "44/44 [==============================] - 5s 106ms/step - loss: 0.1842 - categorical_accuracy: 0.1866 - val_loss: 0.1778 - val_categorical_accuracy: 0.2139 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.1564 - categorical_accuracy: 0.2970 - val_loss: 0.1538 - val_categorical_accuracy: 0.2913 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.1294 - categorical_accuracy: 0.4228 - val_loss: 0.1403 - val_categorical_accuracy: 0.3338 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.1038 - categorical_accuracy: 0.5614 - val_loss: 0.1291 - val_categorical_accuracy: 0.3970 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.0757 - categorical_accuracy: 0.7213 - val_loss: 0.1209 - val_categorical_accuracy: 0.4605 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "44/44 [==============================] - 5s 107ms/step - loss: 0.0507 - categorical_accuracy: 0.7868 - val_loss: 0.1180 - val_categorical_accuracy: 0.4870 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.0322 - categorical_accuracy: 0.8101 - val_loss: 0.1211 - val_categorical_accuracy: 0.4955 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "44/44 [==============================] - 5s 106ms/step - loss: 0.0198 - categorical_accuracy: 0.8194 - val_loss: 0.1272 - val_categorical_accuracy: 0.5059 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.0124 - categorical_accuracy: 0.8200 - val_loss: 0.1354 - val_categorical_accuracy: 0.5137 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "44/44 [==============================] - 6s 134ms/step - loss: 0.0080 - categorical_accuracy: 0.8195 - val_loss: 0.1418 - val_categorical_accuracy: 0.5116 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.0054 - categorical_accuracy: 0.8203 - val_loss: 0.1486 - val_categorical_accuracy: 0.5112 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "44/44 [==============================] - 7s 162ms/step - loss: 0.0038 - categorical_accuracy: 0.8203 - val_loss: 0.1548 - val_categorical_accuracy: 0.5120 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.0028 - categorical_accuracy: 0.8223 - val_loss: 0.1603 - val_categorical_accuracy: 0.5137 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "44/44 [==============================] - 5s 106ms/step - loss: 0.0021 - categorical_accuracy: 0.8238 - val_loss: 0.1646 - val_categorical_accuracy: 0.5155 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "44/44 [==============================] - 5s 104ms/step - loss: 0.0017 - categorical_accuracy: 0.8239 - val_loss: 0.1695 - val_categorical_accuracy: 0.5159 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "44/44 [==============================] - 5s 122ms/step - loss: 0.0013 - categorical_accuracy: 0.8242 - val_loss: 0.1745 - val_categorical_accuracy: 0.5127 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "44/44 [==============================] - 6s 126ms/step - loss: 0.0012 - categorical_accuracy: 0.8258 - val_loss: 0.1751 - val_categorical_accuracy: 0.5155 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "44/44 [==============================] - 5s 107ms/step - loss: 0.0011 - categorical_accuracy: 0.8255 - val_loss: 0.1756 - val_categorical_accuracy: 0.5137 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "44/44 [==============================] - 5s 105ms/step - loss: 0.0011 - categorical_accuracy: 0.8248 - val_loss: 0.1759 - val_categorical_accuracy: 0.5123 - lr: 1.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e9d39c9d0>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_trn, trn[TARGET_COLS], validation_data=(X_val, val[TARGET_COLS]), verbose=True, epochs=20, batch_size=256,\n",
        "          callbacks = [tf.keras.callbacks.ReduceLROnPlateau()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-VWGsOaHJ4r"
      },
      "source": [
        "### F1 Score , Precision and Recall\n",
        "\n",
        "\n",
        "*   F1 Score = 2*((precision*recall)/(precision+recall))\n",
        "*   Recall = True positive yes which means when it is yes, how many times it is actually yes\n",
        "*   Precision = If its yes, how often it's correct \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L4EydSaG5jP",
        "outputId": "d6a61400-776d-4ac5-dd6a-441449f35ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f4e9dc58b90> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f4e9d61d950>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f4e9dc58b90> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f4e9d61d950>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5780682643427743"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "def get_best_thresholds(true, preds):\n",
        "  thresholds = [i/100 for i in range(100)]\n",
        "  best_thresholds = []\n",
        "  for idx in range(25):\n",
        "    f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1) for thresh in thresholds]\n",
        "    best_thresh = thresholds[np.argmax(f1_scores)]\n",
        "    best_thresholds.append(best_thresh)\n",
        "  return best_thresholds\n",
        "\n",
        "val_preds = model.predict(X_val)\n",
        "best_thresholds = get_best_thresholds(val[TARGET_COLS].values, val_preds)\n",
        "for i, thresh in enumerate(best_thresholds):\n",
        "  val_preds[:, i] = (val_preds[:, i] > thresh) * 1\n",
        "f1_score(val[TARGET_COLS], val_preds, average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CblRwEGmHXW-"
      },
      "source": [
        "### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijgpX7KqG5gq"
      },
      "outputs": [],
      "source": [
        "model.save(\"NLP_word_embeddings.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPQ0RpicHbnn"
      },
      "source": [
        "##End of Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86eEuHMS_FR_"
      },
      "source": [
        "# Deployment in FLASK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKbAmrTsG5ZR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h04hfEFSA28p",
        "outputId": "e4a792b8-c982-4a6b-8afe-574d0950ca6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.7.0. in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (3.3.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.44.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.21.6)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (13.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (0.24.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (2.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0.) (1.0.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0.) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0.) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0.) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0.) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0.) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0.) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0.) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0.) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0.) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0.) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0.) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0.) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0.) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0.) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==2.7.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vah3Y2WfG5TC",
        "outputId": "6903012f-44d8-4acc-fa05-c02a8c072c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f4e97abe680> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f4e9d4fe5d0>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f4e97abe680> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Unknown node type <gast.gast.Expr object at 0x7f4e9d4fe5d0>\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        }
      ],
      "source": [
        "maxlen = 200\n",
        "#100000 is the max. no. of words to keep in the tokenized list\n",
        "tok = Tokenizer(num_words = 1000000) \n",
        "tok.fit_on_texts(train['ABSTRACT'].str.lower().tolist() + test['ABSTRACT'].str.lower().tolist())\n",
        "\n",
        "word_embedding_model = load_model(\"NLP_word_embeddings.h5\")\n",
        "\n",
        "demo_data1 = [\"\"\" hello world \"\"\"]\n",
        "\n",
        "\n",
        "demo_input = tok.texts_to_sequences(demo_data1)\n",
        "\n",
        "demo_input =  pad_sequences(demo_input, maxlen=maxlen)\n",
        "\n",
        "pred = word_embedding_model.predict(demo_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvcgIAONCMYe",
        "outputId": "1c0fc547-3d66-4a0f-9f7b-94989da00291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Differential Geometry\n",
            "0.0627366\n"
          ]
        }
      ],
      "source": [
        "print(TARGET_COLS[np.argmax(pred)]) #highest probabilty prediction\n",
        "print(pred[0,np.argmax(pred)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwslv0PdCMe7"
      },
      "outputs": [],
      "source": [
        "def test(text):\n",
        "  demo_input = tok.texts_to_sequences([text])\n",
        "  demo_input =  pad_sequences(demo_input, maxlen=maxlen)\n",
        "  pred = word_embedding_model.predict(demo_input)\n",
        "  # print(TARGET_COLS[np.argmax(pred)], pred[0,np.argmax(pred)])\n",
        "  data = {}\n",
        "  data['topic'] = TARGET_COLS[np.argmax(pred)]\n",
        "  data['confidence'] = str(pred[0,np.argmax(pred)])\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4CpJntEG5GU",
        "outputId": "5841fb66-5b65-4440-baf0-b2fa36c16bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chh2Pj4gFLUK",
        "outputId": "0f50062f-f0e8-45fa-9bb0-90337ceae6f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.15.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.1.4)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.9->flask-cors) (2.0.1)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-3.0.10\n"
          ]
        }
      ],
      "source": [
        "! pip install -U flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku8SY-viFQMG",
        "outputId": "8a7ef10c-91b3-4320-e73d-e94327cf6a85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<flask_cors.extension.CORS at 0x7f4e9d7c8710>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://5c97-35-186-191-181.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [29/Apr/2022 06:13:49] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'topic': 'Astrophysics of Galaxies', 'confidence': '0.7696524'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [29/Apr/2022 06:14:19] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'topic': 'Machine Learning', 'confidence': '0.9853301'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [29/Apr/2022 06:14:52] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'topic': 'Social and Information Networks', 'confidence': '0.37190133'}\n"
          ]
        }
      ],
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, redirect, url_for, request\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "@app.route('/', methods=['POST'])\n",
        "def login():\n",
        "  text = request.form['text']\n",
        "  data = test(text)\n",
        "  print(data);\n",
        "  return data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}